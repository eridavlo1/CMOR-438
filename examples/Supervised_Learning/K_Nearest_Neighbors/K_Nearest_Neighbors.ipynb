{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46b699f",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Example\n",
    "Here is demonstrated the power of Random Forest, a critical ensemble method, by applying them to both a classification task and a regression task using implementations from the rice_ml library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bc4be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, make_regression\n",
    "from rice_ml.supervised_learning.k_nearest_neighbors import KNNClassifier, KNNRegressor\n",
    "from rice_ml.processing.post_processing import accuracy_score, mse, r2_score\n",
    "from rice_ml.processing.preprocessing import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c8c17",
   "metadata": {},
   "source": [
    "## Part 1: KNN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6e3d7",
   "metadata": {},
   "source": [
    "### 2.1 Load Data and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd737e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples in Dataset: 150\n",
      "Number of Features: 4\n",
      "Target Classes: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris Dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Total Samples in Dataset: {X.shape[0]}\")\n",
    "print(f\"Number of Features: {X.shape[1]}\")\n",
    "print(f\"Target Classes: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3200a",
   "metadata": {},
   "source": [
    "### 2.2 Data Pre-Processing: Scaling and Splitting\n",
    "KNN is highly sensitive to feature sclaing. Therefore, we standardize the features (mean=0, variance = 1) before splitting the data for training and testing. This ensures all features contribute equally to the distance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b81d4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Size: 120 samples\n",
      "Testing Set Size: 30 samples\n"
     ]
    }
   ],
   "source": [
    "# Simple Standardization (Z-score scaling)\n",
    "X_mean = np.mean(X, axis = 0)\n",
    "X_std = np.std(X, axis = 0)\n",
    "\n",
    "# Avoiding divisioin by zero for constant features\n",
    "X_std[X_std == 0] = 1\n",
    "X_scaled = (X - X_mean) / X_std\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
    "    X_scaled, y, test_size = 0.2, random_state=67\n",
    ")\n",
    "\n",
    "# Verify the split integrity\n",
    "print(f\"\\nTraining Set Size: {X_train_cls.shape[0]} samples\")\n",
    "print(f\"Testing Set Size: {X_test_cls.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19623af",
   "metadata": {},
   "source": [
    "### 2.3 Initialize and Train the Model\n",
    "Initialize the KNNClassifier using the standard Euclidean metric.\\\n",
    "The KNN model is considered a lazy learner because the training phase involves nothing more than simply storing the training data. The computational work is deferred entirely to the prediction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a99ba690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN is a 'lazy' model. Training involves storing the data...\n",
      "Training Complete. Data stored for prediction.\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the KNearestNeighbors Classifier\n",
    "# Set k=5 (a common odd number) and use the Euclidean distance (default).\n",
    "knn_cls = KNNClassifier(n_neighbors=5, metric='euclidean', weights='distance') \n",
    "\n",
    "print(\"\\nKNN is a 'lazy' model. Training involves storing the data...\")\n",
    "\n",
    "# 2. Fit the model to the training data\n",
    "# Validates and stores X_train and y_train internally.\n",
    "knn_cls.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "print(\"Training Complete. Data stored for prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411160ca",
   "metadata": {},
   "source": [
    "### 2.4 Prediction and Evaluation\n",
    "Use the fitted KNN model to find the 5 nearest neighbors for each test sample, predict its class via majority vote,, and evaluate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a27c6072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction on test set...\n",
      "Prediction Complete.\n",
      "\n",
      "--- Evaluation Results (k=5) ---\n",
      "KNN Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate predictions on the test set\n",
    "print(\"Starting prediction on test set...\")\n",
    "y_pred_cls = knn_cls.predict(X_test_cls)\n",
    "print(\"Prediction Complete.\")\n",
    "\n",
    "# 2. Calculate the Accuracy Score\n",
    "accuracy = accuracy_score(y_test_cls, y_pred_cls)\n",
    "\n",
    "print(f\"\\n--- Evaluation Results (k=5) ---\")\n",
    "print(f\"KNN Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129bae02",
   "metadata": {},
   "source": [
    "## Part 2: KNN Regression\n",
    "Use a synthetic regression dataset to train KNNRegressor, demonstrating prediction of continuous values.\n",
    "### 3.1 Load Data and Preparation\n",
    "Generate a simple dataset where the target variable is continuous. Also, standardize the features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9e5791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Regression Samples: 150\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic regression data\n",
    "X_reg, y_reg = make_regression(n_samples=150, n_features=3, noise=10.0, random_state=67)\n",
    "\n",
    "# Standardization for regression features\n",
    "X_reg_scaled = (X_reg - np.mean(X_reg, axis=0)) / np.std(X_reg, axis=0)\n",
    "\n",
    "print(f\"Total Regression Samples: {X_reg_scaled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb7037",
   "metadata": {},
   "source": [
    "### 3.2 Splitting the Dataset\n",
    "The scaled regression data is split for independent testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2b19adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Size: 120 samples\n"
     ]
    }
   ],
   "source": [
    "# Split the regression dataset\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg_scaled, y_reg, test_size=0.2, random_state=67\n",
    ")\n",
    "\n",
    "# Verification\n",
    "print(f\"\\nTraining Set Size: {X_train_reg.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8454eb",
   "metadata": {},
   "source": [
    "### 3.3 Initialize and Train the Model\n",
    "Initialize the KNNRegressor, which predicts by calculating the weighted average of the neighbor's target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a0f8083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing KNN Regressor...\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the KNNRegressor\n",
    "# Using k=10 with the Manhattan distance and uniform weighting\n",
    "knn_reg = KNNRegressor(\n",
    "    n_neighbors=10, \n",
    "    metric='manhattan', # Testing a different metric\n",
    "    weights='uniform' \n",
    ") \n",
    "\n",
    "print(\"\\nInitializing KNN Regressor...\")\n",
    "\n",
    "# 2. Fit the model (store data)\n",
    "knn_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b84c8",
   "metadata": {},
   "source": [
    "### 3.4 Prediction and Evaluation\n",
    "Predict the continuous targets and evaluate performance using Mean Squared Error (MSE) and R-squared ($R^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c60b9174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Regression Results (k=10, Uniform Weights) ---\n",
      "Mean Squared Error (MSE): 1770.64\n",
      "R-squared (R2) Score: 0.8486\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate continuous predictions\n",
    "y_pred_reg = knn_reg.predict(X_test_reg)\n",
    "\n",
    "# 2. Calculate evaluation metrics\n",
    "\n",
    "mean_squared_error = mse(y_test_reg, y_pred_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"\\n--- Regression Results (k=10, Uniform Weights) ---\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error:.2f}\")\n",
    "print(f\"R-squared (R2) Score: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rice-ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
